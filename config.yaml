Common:
  Results: Results/
  Metrics:
    Name: accuracy
    CachePath: metrics/
  DataSet:
    Name: cifar10
    Label: label
    Path: !ENV ${DATASET_PATH} # Update the dataset path here or add DATASET_PATH to environment.
    Train: '' # Set <>% or <> exact number of batches to train
    Test: '' # Set <>% or <> exact number of batches to test

FineTuning:
  Action: False
  Model:
    Name: google/vit-base-patch16-224-in21k
    CachePath : model/
    OutputPath: tuned-model/
  Hyperparameters:
    TrainBatchSize: 32
    EvalBatchSize: 32
    Epochs: 1
    Lr: 5.e-05
    WeightDecay: 0.0

Distillation:
  Action: False
  Model:
    Name: google/vit-base-patch16-224-in21k
    CachePath: model/
    OutputPath: tuned-model/
    Index: -1   # set -1 to perform distillation with the last fine-tuned model of that dataset otherwise set the index manually.
  StudentModel:
    Name: facebook/deit-base-distilled-patch16-224
    CachePath: model/
    OutputPath: distilled-model/
  Hyperparameters:
    TrainBatchSize: 32
    EvalBatchSize: 32
    Epochs: 1
    Lr: 5.e-05
    WeightDecay: 0.0
  UseDistTokens: True
  DistillationType: soft # Set soft for KL divergence loss using probability or hard for cross entropy loss with logits
  UseAttributionLoss: True
  UseAttentionLoss: False
  UseHiddenLoss : False

Visualization:
  Action: False
  Model:
    Name: google/vit-base-patch16-224
    CachePath: model/
    OutputPath: tuned-model/
    Index: -1
    Device: 'cpu' # update to cuda or mps for GPU support
  Input: images   # kaggle datasets download -d hieu1344/imagenetsample
  Output: featureOutputs/
  Features:
    CompareFeatures: False
    ThresholdScore: 3
    Show: False
    Save: False
  Plot:
    PlotMaskedCurves: False
    ThresholdScore: 2
    BatchSize: 2
    MaskedPerc: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100] # in %